version: '3.3'
services:
  spark-master:
    image: standalone-pyspark:2.3.2-hadoop2.7-py3.7
    ports:
      - "9090:8080"
      - "7077:7077"
      - "4040:4040"
    volumes:
       - ./app:/job/app
       - ./data:/job/data
    environment:
      - SPARK_LOCAL_IP=spark-master
      - SPARK_WORKLOAD=master
  spark-worker-a:
    image: standalone-pyspark:2.3.2-hadoop2.7-py3.7
    ports:
      - "9091:8080"
      - "7000:7000"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-a
    volumes:
       - ./app:/job/app
       - ./data:/job/data

#  spark-worker-b:
#    image: standalone-pyspark:2.3.2-hadoop2.7-py3.7
#    ports:
#      - "9092:8080"
#      - "7001:7000"
#    depends_on:
#      - spark-master
#    environment:
#      - SPARK_MASTER=spark://spark-master:7077
#      - SPARK_WORKER_CORES=1
#      - SPARK_WORKER_MEMORY=300M
#      - SPARK_DRIVER_MEMORY=300M
#      - SPARK_EXECUTOR_MEMORY=300M
#      - SPARK_WORKLOAD=worker
#      - SPARK_LOCAL_IP=spark-worker-b
#    volumes:
#       - ./app:/job/app
#       - ./data:/job/data

  livy:
    image: john/livy-spark:0.3.0
    ports:
      - "8998:8998"
    depends_on:
      - spark-master
      - spark-worker-a
      - spark-worker-b
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_DEPLOY_MODE=client
      - LOCAL_DIR_WHITELIST=/job/data/batches/

    volumes:
     - ./app:/job/app
     - ./data:/job/data

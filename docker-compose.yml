version: '3.3'
services:
  spark-master:
    image: standalone-pyspark:2.3.2-hadoop2.7-py3.7
    ports:
      - "9090:8080"
      - "7077:7077"
      - "4040:4040"
    environment:
      - SPARK_LOCAL_IP=spark-master
      - SPARK_WORKLOAD=master
#    volumes:
#     - ./test_case/data:/job/data

  spark-worker-a:
    image: standalone-pyspark:2.3.2-hadoop2.7-py3.7
    ports:
      - "9091:8080"
      - "7000:7000"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-a
    volumes:
     - ./test_case/data:/job/data

  spark-worker-b:
    image: standalone-pyspark:2.3.2-hadoop2.7-py3.7
    ports:
      - "9092:8080"
      - "7001:7000"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-b
    volumes:
     - ./test_case/data:/job/data

  livy:
    image: john/livy-spark:0.3.0
    ports:
      - "8998:8998"
    depends_on:
      - spark-master
      - spark-worker-a
      - spark-worker-b
    environment:
      - SPARK_MASTER=spark://spark-master:7077
#      - SPARK_DEPLOY_MODE=client
      - LOCAL_DIR_WHITELIST=/job

    volumes:
     - ./test_case:/job
